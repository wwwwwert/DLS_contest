{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5512, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>overview</th>\n",
       "      <th>category_code</th>\n",
       "      <th>tag_list</th>\n",
       "      <th>country_code</th>\n",
       "      <th>num_prev_rounds</th>\n",
       "      <th>has_raised_amount</th>\n",
       "      <th>ln_raised_amount</th>\n",
       "      <th>participants</th>\n",
       "      <th>previous_any_founder_experience</th>\n",
       "      <th>ipo_prob</th>\n",
       "      <th>ma_prob</th>\n",
       "      <th>has_next_round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stackdriver</td>\n",
       "      <td>Stackdriver provides a [powerfully simple moni...</td>\n",
       "      <td>enterprise</td>\n",
       "      <td>application-management, cloud-monitoring, moni...</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.424948</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.182927</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Authix Tecnologies</td>\n",
       "      <td>Authix Tecnologies is a Torino based start-up ...</td>\n",
       "      <td>security</td>\n",
       "      <td>authentication-solution</td>\n",
       "      <td>GRC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.340052</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lytics</td>\n",
       "      <td>Lytics provides B2C marketers the first analyt...</td>\n",
       "      <td>software</td>\n",
       "      <td>analytics, big-data, data-science, bigdata</td>\n",
       "      <td>USA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14.603968</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1World Online</td>\n",
       "      <td>1World Online is a Silicon Valley-based startu...</td>\n",
       "      <td>enterprise</td>\n",
       "      <td>social-research, big-data, analytics, mobile</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.815511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enure Networks</td>\n",
       "      <td>Enure Networks, Ltd. provides home-network man...</td>\n",
       "      <td>software</td>\n",
       "      <td></td>\n",
       "      <td>ISR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.894952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                           overview  \\\n",
       "0         Stackdriver  Stackdriver provides a [powerfully simple moni...   \n",
       "1  Authix Tecnologies  Authix Tecnologies is a Torino based start-up ...   \n",
       "2              Lytics  Lytics provides B2C marketers the first analyt...   \n",
       "3       1World Online  1World Online is a Silicon Valley-based startu...   \n",
       "4      Enure Networks  Enure Networks, Ltd. provides home-network man...   \n",
       "\n",
       "  category_code                                           tag_list  \\\n",
       "0    enterprise  application-management, cloud-monitoring, moni...   \n",
       "1      security                            authentication-solution   \n",
       "2      software         analytics, big-data, data-science, bigdata   \n",
       "3    enterprise       social-research, big-data, analytics, mobile   \n",
       "4      software                                                      \n",
       "\n",
       "  country_code  num_prev_rounds  has_raised_amount  ln_raised_amount  \\\n",
       "0          USA                1                  1         15.424948   \n",
       "1          GRC                1                  1         13.340052   \n",
       "2          USA                2                  1         14.603968   \n",
       "3          USA                1                  1         13.815511   \n",
       "4          ISR                1                  1         15.894952   \n",
       "\n",
       "   participants  previous_any_founder_experience  ipo_prob   ma_prob  \\\n",
       "0             1                                0  0.024390  0.182927   \n",
       "1             1                                0  0.000000  0.000000   \n",
       "2             5                                0  0.021505  0.242105   \n",
       "3             1                                0  0.000000  0.000000   \n",
       "4             2                                0  0.000000  0.000000   \n",
       "\n",
       "   has_next_round  \n",
       "0               1  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('startup_fate/startup_train.csv')\n",
    "data = data.drop(columns=['index'])\n",
    "data = data.fillna('')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('Databook/db-TinyBERT')\n",
    "model = BertModel.from_pretrained(\"Databook/db-TinyBERT\")\n",
    "custom_text = \"You are welcome to utilize any text of your choice.\"\n",
    "encoded_input = tokenizer(custom_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "output_embeddings = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = model(**encoded_input).last_hidden_state.cpu().detach().squeeze().numpy()\n",
    "embedding.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\"Databook/db-TinyBERT\")\n",
    "# model.to(torch.device('cuda'))\n",
    "# tokenizer = BertTokenizer.from_pretrained('Databook/db-TinyBERT')\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "model.to(torch.device('cuda'))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "def bert_embeddings(data: pd.DataFrame, col: str):\n",
    "    embeddings = []\n",
    "    tokenized_texts = tokenizer(data[col].to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    for i in tqdm(range(tokenized_texts['input_ids'].shape[0])):\n",
    "        tokenized = {key: val[[i]].to(torch.device('cuda')) for key, val in tokenized_texts.items()}\n",
    "        embedding = model(**tokenized).last_hidden_state.cpu().detach().squeeze().numpy()\n",
    "        embedding = embedding.sum(axis=0)\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_col(data: pd.DataFrame, col: str, vectorizer=None):\n",
    "    texts_samples = data[col].to_list()\n",
    "    if not vectorizer:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=800, ngram_range=(1, 2))\n",
    "    tf_idf = vectorizer.fit_transform(texts_samples)\n",
    "    return tf_idf, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "text = ['name', 'overview', 'tag_list']\n",
    "categorical = ['category_code', 'country_code']\n",
    "numeric = ['num_prev_rounds', 'ln_raised_amount', 'participants', 'ipo_prob', 'ma_prob']\n",
    "other = ['has_raised_amount', 'previous_any_founder_experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5512/5512 [01:20<00:00, 68.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5512/5512 [01:39<00:00, 55.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5512/5512 [01:16<00:00, 71.88it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_list = []\n",
    "for col in text:\n",
    "    vectorized_texts = bert_embeddings(data, col)\n",
    "    vectors_list.append(vectorized_texts)\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    # ('scaling', StandardScaler(), numeric),\n",
    "    ('other',  'passthrough', other)\n",
    "])\n",
    "\n",
    "target = data['has_next_round'].to_numpy()\n",
    "features = column_transformer.fit_transform(data).toarray()\n",
    "features = np.concatenate([features, *vectors_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.8, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calc_score(true, pred):\n",
    "    return 32 * (accuracy_score(true, pred) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.045715\n",
      "0:\tlearn: 0.6048991\ttest: 0.5793291\tbest: 0.5793291 (0)\ttotal: 321ms\tremaining: 5m 21s\n",
      "100:\tlearn: 0.7827172\ttest: 0.6137806\tbest: 0.6264733 (11)\ttotal: 21.9s\tremaining: 3m 14s\n",
      "200:\tlearn: 0.8772964\ttest: 0.6137806\tbest: 0.6264733 (11)\ttotal: 43.4s\tremaining: 2m 52s\n",
      "300:\tlearn: 0.9614425\ttest: 0.5965549\tbest: 0.6264733 (11)\ttotal: 1m 4s\tremaining: 2m 30s\n",
      "400:\tlearn: 0.9879791\ttest: 0.5947416\tbest: 0.6264733 (11)\ttotal: 1m 26s\tremaining: 2m 9s\n",
      "500:\tlearn: 0.9984123\ttest: 0.6029012\tbest: 0.6264733 (11)\ttotal: 1m 48s\tremaining: 1m 47s\n",
      "600:\tlearn: 0.9995464\ttest: 0.5965549\tbest: 0.6264733 (11)\ttotal: 2m 9s\tremaining: 1m 26s\n",
      "700:\tlearn: 1.0000000\ttest: 0.5865820\tbest: 0.6264733 (11)\ttotal: 2m 31s\tremaining: 1m 4s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m catboost_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# 'task_type': 'GPU',\u001b[39;00m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcatboost_features\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     15\u001b[0m calc_score(y_test, pred)\n",
      "File \u001b[0;32m/media/black_chick/Notes/HSE/CV_course/cv_env/lib/python3.9/site-packages/catboost/core.py:5201\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   5199\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 5201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5202\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5203\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/media/black_chick/Notes/HSE/CV_course/cv_env/lib/python3.9/site-packages/catboost/core.py:2396\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2393\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2405\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m/media/black_chick/Notes/HSE/CV_course/cv_env/lib/python3.9/site-packages/catboost/core.py:1776\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4833\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4882\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost_features = {\n",
    "    'iterations': 1000,\n",
    "    'random_seed': 1,\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'verbose': 100,\n",
    "    # 'task_type': 'GPU',\n",
    "}\n",
    "model = CatBoostClassifier(\n",
    "    **catboost_features\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "calc_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
